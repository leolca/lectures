\subsection{Informação}
\begin{frame}%[allowframebreaks]
  \frametitle{Informação}
  O conceito de informação é amplo, sendo difícil ser contemplado em sua plenitude por qualquer definição.

  \vspace{2ex}
  \citet{shannon1948} propôs a definição de \textit{entropia} que possui
  muitas propriedades em comum o senso comum do que deve ser informação.

  \vspace{2ex}
  A informação fornecida por uma mensagem corresponde com o quão improvável é esta mensagem.

\end{frame}
\note{
O que é previsível fornece pouca ou nenhuma informação.

Quanto mais incerto, mais informação há.
}

\begin{frame}%[allowframebreaks]
  \frametitle{Informação}
  \citet{hartley1928} propõem uma medida de informação para uma variável aleatória $X$:
  \begin{equation}
    I(X) = \log_b L ,
  \end{equation}
  onde $L$ é o número de possíveis valores que $X$ pode assumir.
  Se $b=2$, a informação será medida em `bits' (nome sugerido por J.W. Tukey).
\end{frame}
\note{
  A definição de Hartley é condizente com as seguintes intuições sobre informação: 
  \begin{itemize}
  \item Dois cartões de memória devem possuir o dobro da capacidade de um cartão para
  armazenamento de informação.
  \item Dois canais de comunicação idênticos devem possuir o dobro da capacidade 
  de transmitir informação que um único canal.
  \item Um dispositivo com duas posições estáveis, como um relé ou um flip-flop,
  armazena um bit de informação. $N$ dispositivos deste tipo podem armazenar $N$ bits
  de informação, já que o número total de estados é $2^N$ e $\log_2 2^N = N$.
  \end{itemize}

  Entretanto, isto é válido apenas quando as mensagens/eventos são equiprováveis. 
  No caso extremo, note que se o cartão de memória armazena apenas zeros, ele não
  é capaz de armazenar informação alguma.
}

\begin{frame}%[allowframebreaks]
  \frametitle{Entropia}
  Suponha que existam eventos $E_k$ com probabilidade de ocorrência $p_k$. 

  \begin{itemize}
  \item Shannon: informação associada ao evento $E_k$ é dada por $I(E_k) = \log (1/p_k)$.
        \begin{itemize}
        \item Se $p_k=1$ $\rightarrow$ não há surpresa na ocorrência do evento $E_k$.
        \item Se $p_k=0$ $\rightarrow$ surpresa infinita, afinal o evento $E_k$ é impossível.
        \item $I(E_k) = - \log p(E_k)$ é a auto-informação do evento ou mensagem $E_k$.
        \end{itemize}
  \item \textbf{Sempre} utilizaremos a base $2$ para o cálculo do logaritmo, desta forma
  $\log \equiv \log_2$, a menos que seja especificado o contrário.
  \item $\ln$ é o logaritmo na base natural $e$.
  \end{itemize}
\end{frame}

\begin{frame}%[allowframebreaks]
  \frametitle{Entropia}
  \begin{itemize}
  \item Notação: $p(x) = P_X(X=x)$, a probabilidade do evento $\{X=x\}$, da v.a. $X$ assumir o valor $x$.
  \item Valor esperado da v.a. $X$: $E[X] = EX = \sum_x x p(x)$.
  \item Dada uma função $g: \mathcal{X} \rightarrow \mathbb{R}$, o valor esperado da
  v.a. $g(X)$ é $E g(X) = \sum_x g(x) p(x)$.
  \item Considere $g(x) = \log(1/p(x))$. Então $g(x)$ é a imprevisão (surpresa) de encontrar o evento $X=x$.
        Tomando o valor esperado de $g$ teremos
        \begin{equation}
        \sum_x p(x) \log \frac{1}{p(x)} ,
        \end{equation}
        ou seja, a esperança da surpresa, ou o valor esperado da imprevisão na variável aleatória $X$.
        Esta é a definição de entropia.
  \end{itemize}
\end{frame}

