\subsection{Entropia e Informação Mútua}
% ~/ee/ufsj/2016_01/ti/prova/prova-subs.tex 

\begin{questions}
\question{
  Uma fonte produz símbolos aleatórios $X \in \mathcal{X} = \{a,b,c,d\}$ que serão 
  enviados através de um canal de comunicação ruidoso. A saída do canal de 
  comunicação é a variável aleatória $Y \in \mathcal{Y}=\{a,b,c,d\}$. 
  A distribuição conjunta entre essas duas v.a.s é a seguinte

  \begin{tabular}{ c c c c c}
          & $x=a$  & $x=b$  & $x=c$  & $x=d$ \\
   $y=a$  & $1/8$  & $1/16$ & $1/16$ & $1/4$ \\
   $y=b$  & $1/16$ & $1/8$  & $1/16$  & $0$   \\
   $y=c$  & $1/32$ & $1/32$ & $1/16$ & $0$   \\
   $y=d$  & $1/32$ & $1/32$ & $1/16$ & $0$ 
  \end{tabular} 

  \begin{parts}
  \part Encontre a distribuição marginal de $X$ e calcule a entropia marginal $H(X)$ em bits.
  \part Encontre a distribuição marginal de $Y$ e calcule a entropia marginal $H(Y)$ em bits.
  \part Calcule a entropia conjunta $H(X,Y)$ em bits.
  \part Calcule a entropia condicional $H(Y|X)$ em bits.
  \part Calcule a informação mútua $I(X;Y)$ em bits.
  \end{parts}
}

\begin{solution}
\begin{parts}
\part to-do
\part to-do
\part to-do
\part to-do
\part to-do
\end{parts}
\end{solution}
\end{questions}
