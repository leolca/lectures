\subsection{Entropia de uma fonte discreta}
% ~/MC-Projects/prova-ti-201602-subs/teoricas.tex

\begin{questions}
\question{
Sobre a entropia de uma fonte discreta podemos afirmar que:
\begin{choices}
\CorrectChoice A entropia máxima ocorre quando a distribuição da fonte for uniforme.
\choice A entropia mínima ocorre quando a distribuição da fonte for uniforme.
\choice A entropia mínima ocorre quando a distribuição da fonte for normal. 
\choice A entropia máxima ocorre quando a distribuição da fonte for normal.
\choice A entropia máxima ocorre quando a distribuição da fonte for d-ádica.
\end{choices} 
}

\question{
Sobre a entropia de uma fonte discreta podemos afirmar que:
    \begin{choices}
      \correctchoice{a entropia nunca será negativa.}
      \choice{a entropia nunca será nula.}
      \choice{a entropia nunca será positiva.}
      \choice{a entropia nunca será maior do que a informação mútua.}
      \choice{a entropia nunca será maior do que a cardinalidade do alfabeto da fonte.}
    \end{choices}

}

\question{
    A entropia é uma função:
    \begin{choices}
      \correctchoice{estritamente côncava.}
      \choice{estritamente convexa.}
      \choice{nem côncava, nem convexa.}
      \choice{linear nos parâmetros da distribuição da fonte.}
      \choice{linear por partes.}
    \end{choices}
}

\question{
    A divergência (KL) entre uma dada função massa de probabilidade de uma
        v.a. e  distribuição uniforme é igual a:
    \begin{choices}
      \correctchoice{logaritmo da cardinalidade do alfabeto menos a entropia da v.a..}
      \choice{cardinalidade do alfabeto mais a entropia da v.a..}
      \choice{informação mútua entre a v.a. e a distribuição uniforme.}
      \choice{cardinalidade do alfabeto menos a entropia da distribuição uniforme.}
      \choice{logaritmo da cardinalidade do alfabeto mais a entropia da distribuição uniforme.}
    \end{choices}
}
\end{questions}
