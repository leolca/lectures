\subsection{Entropia Conjunta}
\begin{frame}%[allowframebreaks]
  \frametitle{Entropia Conjunta}
  Duas variáveis aleatórias $X$ e $Y$ possuem \textbf{entropia conjunta}
  \begin{equation}
  H(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) = E \log \frac{1}{p(X,Y)} .
  \end{equation}

  Generalizando para vetores $X_{1:N} = (X_1, X_2, \ldots, X_N)$
  \begin{eqnarray}
  H(X_{1:N}) &=& H(X_1, X_2, \ldots, X_N) \nonumber \\
       &=& \sum_{x_1, x_2, \ldots , x_N} p(x_1, \ldots, x_N) \log \frac{1}{p(x_1, \ldots, x_N)} \nonumber \\
       &=& E \log \frac{1}{p(X_1, \ldots, X_N)}
  \end{eqnarray}
\end{frame}



\subsection{Entropia Condicional}
\begin{frame}[allowframebreaks]
  \frametitle{Entropia Condicional}
  Dadas duas v.a. $X$ e $Y$ relacionadas por $p(x,y)$, conhecer o evento $X=x$ pode alterar a entropia de $Y$.

  \begin{itemize}
  \item Entropia condicionada a um evento $H(Y|X=x)$
     \begin{eqnarray}
     H(Y|X=x) &=& E \log \frac{1}{p(Y|X=x)} \nonumber \\
              &=& - \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x)
     \end{eqnarray}

  \item $H(Y|X=x)$ é uma função de $X$. Podemos então tomar seu valor esperado $E[H(Y|X=x)]$
  e obter a entropia condicional $H(Y|X)$.

  \item Realizando a média sobre todos os $x$, obteremos a entropia condicional $H(Y|X)$.
     \begin{eqnarray}
     H(Y|X) &=& \sum_x p(x) H(Y|X=x) \nonumber \\
        &=& - \sum_x p(x) \sum_y p(y|x) \log p(y|x) \nonumber \\
        &=& - \sum_{x,y} p(x,y) \log p(y|x) \nonumber \\
        &=& E \log \frac{1}{p(Y|X)}
     \end{eqnarray}
  \end{itemize}
\end{frame}


\subsection{Regra da Cadeia}
\begin{frame}%[allowframebreaks]
  \frametitle{Regra da Cadeia}
  \begin{theorem}[Regra da Cadeia para a Entropia]
  \begin{equation}
  H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
  \end{equation}
  \end{theorem} 
  
  \begin{proof}
  \begin{equation}
  - \log p(x,y) = - \log p(x) - \log p(y|x)
  \end{equation}
  tomando o valor esperado de ambos os lados, obtemos o resultado desejado.
  \end{proof}

  \begin{corollary}
  Se $X \independent Y$ então $H(X,Y) = H(X) + H(Y)$.
  \end{corollary}
\end{frame}


\begin{frame}%[allowframebreaks]
  \frametitle{Regra da Cadeia}
  \begin{proof}[regra da cadeia]
  \begin{eqnarray}
  H(X,Y) &=& - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)  \log p(y|x) p(x) \nonumber \\
        &=& - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)  \log p(y|x) - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)  \log p(x) \nonumber \\
        &=& H(Y|X) - \sum_{x \in \mathcal{X}} \log p(x) \sum_{y \in \mathcal{Y}} p(x,y) \nonumber \\
        &=& H(Y|X) - \sum_{x \in \mathcal{X}} \log p(x) (p(x)) \nonumber \\
        &=& H(Y|X) + H(X)
  \end{eqnarray}
  \end{proof}
\end{frame}
\note{
Exemplo Canal de Comunicação
\vspace{3ex}

   Suponha um canal de comunicação com entrada $X$ e saída $Y$.

   $H(X|Y)$ pode ser visto como a incerteza sobre $X$ (a mensagem enviada) quando 
   $Y$ (a mensagem recebida) for conhecido.

   Sem nenhuma observação no processo de comunicação através deste canal, o
   receptor não sabe nada sobre $X$ nem $Y$, assim a incerteza inicial é $H(X,Y)$.
   Quando o receptor recebe a mensagem $Y$, ele ganha uma quantidade de informação $H(Y)$.
   Assim a informação que falta sobre $X$ mesmo conhecendo $Y$ é dada por
   $H(X|Y) = H(X,Y) - H(Y)$. Esta pode ser tido como uma medida do erro na comunicação.
   A quantidade de informação que o receptor de fato ganha é $I(X;Y) = H(X) - H(X,Y)$.

}

\begin{frame}[allowframebreaks]
  \frametitle{Regra da Cadeia Generalizada}
  \begin{theorem}[Regra da Cadeia para a Entropia]
  \begin{equation}
  H(X_1, X_2, \ldots, X_N) = \sum_{i=1}^N H(X_i | X_1, X_2, \ldots, X_{i-1}) 
  \end{equation}
  \end{theorem}

  \begin{eqnarray}
  H(X_1, X_2, \ldots, X_N) &=& H(X_1) + H(X_2 | X_1) + \\
        && H(X_3 | X_1, X_2) + H(X_4 | X_1, X_2, X_3) + \cdots \nonumber
  \end{eqnarray}
  
  \framebreak
  \begin{proof}
  Utilizando a regra da cadeia da probabilidade condicional, teremos
  \begin{equation}
  p(x_1, x_2, \ldots, x_N) = \prod_{i=1}^N p(x_i | x_1, \ldots , x_{i-1}) ,
  \end{equation}
  então
  \begin{equation}
  - \log p(x_1, x_2, \ldots, x_N) = - \sum_{i=1}^N \log p(x_i | x_1, x_2, \ldots , x_{i-1}) 
  \end{equation}
  tomando o valor esperado de ambos os lados, obtemos o resultado desejado.
  \end{proof}
\end{frame}


